# -*- coding: utf-8 -*-
"""Kopie von Framingham.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fRiSYIyUC7RwU0AhupxBgiGpj_8vK75s
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import seaborn as sns
import sklearn as sk
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
pd.options.display.max_columns = None

df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/framingham.csv")
df.rename({'male': 'sex'}, axis=1, inplace=True)
print(df.isnull().sum())

df.head()

y=df.TenYearCHD
x=df#.drop('TenYearCHD', axis=1)
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=5)
TenYearCHD_test=xtrain.TenYearCHD
xtrain=xtrain.drop('TenYearCHD', axis=1)
xtrain = pd.DataFrame(xtrain)
xtrain = xtrain.sort_index()
print(xtrain.dtypes)

print(xtrain.isnull().sum())

"""# DataCleaning - General"""

#create AgeRange categories to fill nan values by category
#bins = [0, 30, 40, 50, 60, 70, np.inf]
#labels = ['0-30', '30-40', '40-50', '50-60', '60-70', '70+']

#xtrain['AgeRange'] = xtrain.cut(xtrain['age'], bins, labels=labels)

#print(xtrain.dtypes)
#print(xtrain)

#xtrain.shape
#df.isnull().sum()

"""# DataCleaning - BPMeds, CigsperDay, totChol"""

print(xtrain.BPMeds.value_counts())
# fill NaN values in BPMeds column with 0 
xtrain.BPMeds.fillna(0,inplace=True)

#cigs per day
null_data_cpd = xtrain[xtrain.isnull().cigsPerDay]
print(null_data_cpd[["currentSmoker","cigsPerDay"]])
#all the missing values of cigsperday are from people who are actually smoking, so we can fill missing values with mean of smokers
xtrain.cigsPerDay.fillna(xtrain.cigsPerDay.mean(),inplace=True)

#totChol
#fill the missing values with the mean
xtrain.totChol.fillna(xtrain.totChol.mean(),inplace=True)

"""# DataCleaning - Heart Rate"""

#heartrate
xtrain.dropna(subset = ["heartRate"], inplace=True)
print(xtrain.isnull().sum())

"""# DataCleaning - Eductaion, BMI"""

#set education to 1
xtrain.loc[:,'education'] = '1'
df.head()

i = 1
VAR_1 = 19
VAR_2 = 24
VAR_max = xtrain['age'].max()

while i < 7:

  if VAR_1 == 19: # Start Age of 19
      mean_BMI = xtrain[(xtrain.age >= VAR_1) & (xtrain.age <= VAR_2)].BMI.mean()
      mean_BMI = round(mean_BMI, 2)
      xtrain['BMI']= np.where((xtrain.age >= VAR_1) & (xtrain.age <= VAR_2) & (xtrain.BMI.isnull()), mean_BMI, xtrain.BMI)
      
  elif VAR_1 >= 25 and VAR_1 <= 54:
      mean_BMI = xtrain[(xtrain.age >= VAR_1) & (xtrain.age <= VAR_2)].BMI.mean()
      mean_BMI = round(mean_BMI, 2)
      xtrain['BMI']= np.where((xtrain.age >= VAR_1) & (xtrain.age <= VAR_2) & (xtrain.BMI.isnull()), mean_BMI, xtrain.BMI)

  elif VAR_1 == 55:
      mean_BMI = xtrain[(xtrain.age >= VAR_1) & (xtrain.age <= VAR_2)].BMI.mean()
      mean_BMI = round(mean_BMI, 2)
      xtrain['BMI']= np.where((xtrain.age >= VAR_1) & (xtrain.age <= VAR_2) & (xtrain.BMI.isnull()), mean_BMI, xtrain.BMI)

  elif VAR_1 == 66:
      mean_BMI = xtrain[(xtrain.age >= 66)& (xtrain.age <= VAR_max)].BMI.mean()
      mean_BMI = round(mean_BMI, 2)
      xtrain['BMI']= np.where((xtrain.age >= VAR_1) & (xtrain.age <= VAR_max) & (xtrain.BMI.isnull()), mean_BMI, xtrain.BMI)

  print('Mean of BMI(NRC): ',mean_BMI,'\tAge from: ', VAR_1,' to ', VAR_2)

  if i == 1:
     VAR_1 = 25
     VAR_2 = 34
  elif i >= 2 and i <= 3:
     VAR_1 = VAR_1 + 10
     VAR_2 = VAR_2 + 10
  elif i == 4:
     VAR_1 = 55
     VAR_2 = 65
  elif i == 5:
     VAR_1 = 66
     VAR_2 = VAR_max 

  i += 1

xtrain.isnull().sum()

#fill missing values of BMI with mean value of the category
#define age groups
#bins= [30,44,57,71]
#labels = ['30-44','45-57','58-71']
#xtrain['AgeGroup'] = pd.cut(xtrain['age'], bins=bins, labels=labels, right=False)
#print (xtrain)

#print(xtrain.groupby('AgeGroup')['BMI'].mean())

#BMI1=xtrain[xtrain['AgeGroup']=='30-44'].BMI.median()
#BMI2=xtrain[xtrain['AgeGroup']=='45-57'].BMI.median()
#BMI3=xtrain[xtrain['AgeGroup']=='58-71'].BMI.median()

#print(BMI1)
#print(BMI2)
#print(BMI3)

#xtrain[xtrain['AgeGroup'] == '30-44']['BMI'].fillna(BMI1)
#xtrain[xtrain['AgeGroup'] == '45-57']['BMI'].fillna(BMI2)
#xtrain[xtrain['AgeGroup'] == '58-71']['BMI'].fillna(BMI3)


#if 'AgeGroup' == '30-44':
    #xtrain['BMI'].fillna(BMI1, inplace=True)
#elif 'AgeGroup' == '45-57':
    #xtrain['BMI'].fillna(BMI2, inplace=True)
#else:
    #xtrain['BMI'].fillna(BMI3, inplace=True)

#print(xtrain[xtrain['BMI'].isnull()])
#print(xtrain['BMI'].isnull().sum())

#Category 30-40
#df.loc[(df['AgeRange']=="30-40"), 'BMI'].mean()
#df.loc[(df['AgeRange']=="30-40"), 'BMI'] = df.BMI.fillna(df.loc[(df['AgeRange']=="30-40"), 'BMI'].mean())
#print(df[df['AgeRange'] == '30-40'].isnull().sum())
#df[df['AgeRange'] == '30-40'].head()

"""# K Nearest Neighbors for glucose"""

#save missing values of glucose in seperate dataset
#null_data_glucose = xtrain[df.isnull().glucose]
#null_data_glucose
#null_data_glucose = null_data_glucose.drop("glucose", axis=1)
#null_data_glucose
#null_data_glucose = null_data_glucose.values
#print(null_data_glucose)
test_df=xtrain[xtrain['glucose'].isnull()]
test_df

#k-neartes neighbors
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import KNeighborsRegressor
data = xtrain.dropna()

#define distances on the vectors of the independent variables, so first get pandas DataFrame into a NumPy array
X = data.drop("glucose", axis=1)
X = X.values    #independet variables
y = data["glucose"]
y = y.values  #dependet variable of the model

print(X.shape, y.shape)

from sklearn.model_selection import train_test_split
X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size=0.3, random_state=1)
print(X_test1.shape, y_test1.shape)

#Data Standardization
from sklearn.preprocessing import StandardScaler

# define standard scaler
scaler = StandardScaler()

# transform data
X_train1 = scaler.fit_transform(X_train1) #only fit the training set
X_test1 = scaler.transform(X_test1)

from sklearn.model_selection import GridSearchCV
parameters = {"n_neighbors": range(1, 31),
              "weights": ["uniform","distance"],
              "metric": ["euclidean", "manhattan"]}
              
gs=GridSearchCV(KNeighborsRegressor(), parameters, scoring="neg_mean_squared_error",cv=10, n_jobs=-1)

#fitting the model for grid
gs_results = gs.fit(X_train1, y_train1)

print(gs_results.best_params_)
#beste metrik durch GridSearch

print(gs_results.best_score_) #andere metrik

#Cross Validation
#import k-folder
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsRegressor
knn = KNeighborsRegressor(n_neighbors = 16)

# X,y will automatically devided by 5 folder, the scoring I will still use the accuracy
score = cross_val_score(knn, X, y, cv=5, scoring="neg_root_mean_squared_error")
#takes the features X (independet variables) and target y (dependent variable) -> splits into k-folds (cv parameter)
#Model is trained using K-1 of the folds as training data
print(score)

#average of these five scores to get accuracy score
print("The accuracy of the model is : ", -score.mean())

#Fit Knn Model based on grid Search findings
knn = KNeighborsRegressor(n_neighbors=16, weights='distance', metric='euclidean')
knn.fit(X_train1, y_train1)

#Model evaluation
from sklearn.metrics import mean_squared_error
from math import sqrt
y_preds = knn.predict(X_test1)
mse = mean_squared_error(y_test1, y_preds)
rmse = sqrt(mse)
rmse

y_preds=pd.DataFrame(y_preds)
print(y_preds.head())

print(xtrain.isnull().sum())
y_train2=data.glucose
x_train2=data.drop(['glucose'], axis=1)

##Train knn on whole xtrain and define rows with missing glucose values as x_test
x_test2=xtrain[xtrain['glucose'].isnull()].drop(['glucose'], axis=1)
print(x_test2)
print(x_train2, y_train2)

#Standardization
x_train2=scaler.fit_transform(x_train2)
x_test2=scaler.transform(x_test2)

#knn
knn = KNeighborsRegressor(n_neighbors=18, weights='distance', metric='euclidean')
knn.fit(x_train2, y_train2)
y_pred2 = knn.predict(x_test2)
y_pred2 = pd.DataFrame(y_pred2)

y_pred2['glucose']=y_pred2
y_pred2=y_pred2.drop(0, axis=1)
print(y_pred2)

#Fehlende Werte mit y_pred1 ersetzen
index_NaN = xtrain['glucose'].index[xtrain['glucose'].apply(np.isnan)].tolist()
print(index_NaN)

for index, value in enumerate(y_pred2['glucose']):
    xtrain.at[index_NaN[index], 'glucose'] = value

print(xtrain['glucose'])

print(xtrain.isnull().sum())

xtrain['TenYearCHD']=ytrain
xtrain.head()

xtrain.info()

xtrain.to_csv('framingham_cleaned_0508.csv')



#X = scaler.fit_transform(X)

#Fit knn model on whole dataset
#knn1 = KNeighborsRegressor(n_neighbors=16, weights='distance', metric='euclidean')
#knn1.fit(X, y)

#Use Knn to predict the nan values of glucose
#null_data_glucose = scaler.transform(null_data_glucose) 
#gluc_preds = knn1.predict(null_data_glucose)
#gluc_preds
#np.count_nonzero(gluc_preds)

#glu_nan = xtrain['glucose'].isna()
#glu_nan
#glu_nan.value_counts()

#xtrain.loc[glu_nan, 'glucose'] = glu_nan

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import confusion_matrix, classification_report
pd.options.display.max_columns = None

print(xtrain.isnull().sum())



"""# Linear Regression for Glucose"""

from sklearn import linear_model
from sklearn.metrics import mean_squared_error, r2_score

regr = linear_model.LinearRegression()

# Train the model using the training sets
regr.fit(X_train, y_train)

#evaluate the fit on the testing data
y_pred_lin = regr.predict(X_test)

#print('Coefficients: \n', regr.coef_)
# The mean squared error
mse_lin_test = (mean_squared_error(y_test, y_pred_lin))
rmse = sqrt(mse_lin_test)
print(rmse)

# The coefficient of determination: 1 is perfect prediction
print(r2_score(y_test, y_pred_lin))

#evaluate the fit on the training data
y_pred_lin1 = regr.predict(X_train)

#print('Coefficients: \n', regr.coef_)
# The mean squared error
mse_lin_train = (mean_squared_error(y_train, y_pred_lin1))
rmse1 = sqrt(mse_lin_train)
print(rmse1)
# The coefficient of determination: 1 is perfect prediction
print(r2_score(y_train, y_pred_lin1))

"""# KNN Try outs"""

correlation_matrix = df.corr()
correlation_matrix["glucose"]

#Plotting the model
#create a scatter plot of the first and second columns of X_test by subsetting the arrays X_test[:,1] and X_test[:,10]
#columns are age and sysBP --> correlated
#colorbar shows predicted glucose value
import seaborn as sns
cmap = sns.cubehelix_palette(as_cmap=True)
f, ax = plt.subplots()
points = ax.scatter(X_test[:, 1], X_test[:, 10], c=test_preds, s=50, cmap=cmap)
f.colorbar(points)
plt.show()

f, ax = plt.subplots()
points = ax.scatter(X_train[:, 1], X_train[:, 10], c=train_preds, s=50, cmap=cmap)
f.colorbar(points)
plt.show()

# Commented out IPython magic to ensure Python compatibility.
#choose differenct neighbors to see which K is the best K
import matplotlib.pyplot as plt 
# %matplotlib inline

# choose k between 1 to 100
k_range = range(1, 31)

k_scores = []

# use iteration to caclulator different k in models, then return the average accuracy based on the cross validation
for k in k_range:
    knn = KNeighborsRegressor(n_neighbors=k)
    scores = cross_val_score(knn, X, y, cv=5, scoring="neg_root_mean_squared_error")
    k_scores.append(scores.mean())

# plot to see clearly
plt.plot(k_range, k_scores)
plt.xlabel('Value of K for KNN')
plt.ylabel('Cross-Validated Accuracy')
plt.show()
